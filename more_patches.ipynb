{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/marekpiotradamczyk/ml_uwr_22/blob/main/kmeans_deep_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it differs from the default solution? \n",
    "\n",
    "More smaller patches and denser patches extraction from images.\n",
    "\n",
    "    PATCH_SIZE = 4\n",
    "    patch_num  = 5000000\n",
    "    STRIDE     = 2\n",
    "    \n",
    "Also, more clustsers:\n",
    "\n",
    "    kroot = 16\n",
    "    k = kroot * kroot\n",
    "    \n",
    "Note that it takes significant time to execute all steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & default imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T36fIpVCGGAt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sstats\n",
    "import multiprocessing as mp\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import animation, pyplot, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import httpimport\n",
    "from os.path import join\n",
    "import os.path\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f6je-NQf810D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown httpimport\n",
    "![ -e cifar.npz ] || gdown 'https://drive.google.com/uc?id=1oBzZdtg2zNTPGhbRy6DQ_wrf5L5OAhNR' -O cifar.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XVdY0QLv9dAv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with np.load('cifar.npz') as data:\n",
    "    cifar_train_data = data['train_data']\n",
    "    cifar_train_labels = data['train_labels']\n",
    "    cifar_test_data = data['test_data']\n",
    "    cifar_test_labels = data['test_labels']\n",
    "\n",
    "\n",
    "def exists(file_path):\n",
    "    return os.path.isfile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GoayFo-HlVXf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_trn = cifar_train_data\n",
    "y_trn = cifar_train_labels\n",
    "X_tst = cifar_test_data\n",
    "y_tst = cifar_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krVBYnbQS5Z_"
   },
   "source": [
    "# Deep Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifx-N_fsS5aA"
   },
   "source": [
    "## find important patterns in patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 6\n",
    "patch_num  = 100000\n",
    "STRIDE     = 8\n",
    "kroot = 16\n",
    "k = kroot **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clf():\n",
    "      \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def contrast(image):\n",
    "        return (image-image.min())/(image.max() - image.min())\n",
    "\n",
    "    def normalize_patch(patch, eps=10):\n",
    "        return (patch - patch.mean())/np.sqrt(patch.var() + eps)\n",
    "\n",
    "    def whiten(X):\n",
    "        X_norm = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "        cov = np.cov(X_norm, rowvar=False) \n",
    "        U,S,V = np.linalg.svd(cov)\n",
    "\n",
    "        X_zca = U.dot(np.diag(1.0/np.sqrt(S + 0.1))).dot(U.T).dot(X_norm.T).T\n",
    "        return X_zca\n",
    "\n",
    "    def centroids(data):\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, verbose=False, n_init=1, max_iter=200, batch_size=10000)\n",
    "        kmeans.fit(data)\n",
    "        return kmeans.cluster_centers_\n",
    "    \n",
    "    def extract_patches(data):\n",
    "        patches = []\n",
    "        reshaped = data.reshape(-1,32,32,3)\n",
    "        n = int(patch_num / (32-PATCH_SIZE+1) ** 2 + 1)\n",
    "        for i in tqdm(range(n)):\n",
    "            for r in range(32-PATCH_SIZE+1):\n",
    "                for c in range(32-PATCH_SIZE+1):\n",
    "                    patch = reshaped[i][c:(c+PATCH_SIZE),r:(r+PATCH_SIZE)].flatten()\n",
    "                    patch_norm = normalize_patch(patch, eps=10)\n",
    "                    patches.append(patch_norm)\n",
    "\n",
    "        P = np.vstack(patches)\n",
    "        return whiten(P)\n",
    "    \n",
    "    def dist(x,y):\n",
    "        return np.sqrt((x - y).dot(x-y))\n",
    "\n",
    "    def create_patch_features(X):    \n",
    "        X_mapped_list_per_image = []\n",
    "        for i in tqdm(range(X.shape[0])):\n",
    "            mapped_features = []\n",
    "            for r in range(0, 32-PATCH_SIZE+1, STRIDE):\n",
    "                for c in range(0, 32-PATCH_SIZE+1, STRIDE):\n",
    "                    patch = X[i].reshape(32,32,3)[c:(c+PATCH_SIZE),r:(r+PATCH_SIZE)].flatten()\n",
    "                    patch_norm = normalize_patch(patch, eps=0.01)\n",
    "                    mapped_features.append([dist(patch_norm, f) for f in filters_final])\n",
    "            X_mapped_list_per_image.append(np.vstack(mapped_features))\n",
    "        X_mapped = np.asarray(X_mapped_list_per_image).reshape(-1, ((32-PATCH_SIZE)//STRIDE+1)**2*filters_final.shape[0])\n",
    "        return X_mapped\n",
    "\n",
    "    def create_patch_features__vectorized(X):    \n",
    "        X_mapped_list_per_image = []\n",
    "        for i in tqdm(range(X.shape[0])):\n",
    "            patches = image.extract_patches_2d(X[i], (PATCH_SIZE, PATCH_SIZE))\n",
    "            strided_patches = patches.reshape( 32-PATCH_SIZE+1 , 32-PATCH_SIZE+1, PATCH_SIZE, PATCH_SIZE, 3)[::STRIDE,::STRIDE,:,:,:]\n",
    "            strided_patches = strided_patches.reshape(((32-PATCH_SIZE)//STRIDE+1)**2, PATCH_SIZE * PATCH_SIZE * 3)\n",
    "            mapped_features = euclidean_distances(np.asarray([normalize_patch(patch, eps=0.01) for patch in strided_patches]), filters_final)\n",
    "            X_mapped_list_per_image.append(mapped_features.reshape(((32-PATCH_SIZE)//STRIDE+1)**2 * filters_final.shape[0]))\n",
    "        X_mapped = np.asarray(X_mapped_list_per_image)\n",
    "        return X_mapped\n",
    "\n",
    "    \n",
    "    def fit(X_trn, y_trn):\n",
    "        P_zca = extract_patches(X_trn)\n",
    "        filters_final = run_cached(centroids, P_zca, 'kmeans')\n",
    "        X_mapped_trn = run_cached(create_patch_features__vectorized, X_trn, 'X_mapped_trn')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2-Ug1JOeS5aC"
   },
   "outputs": [],
   "source": [
    "def contrast(image):\n",
    "    return (image-image.min())/(image.max() - image.min())\n",
    "\n",
    "def normalize_patch(patch, eps=10):\n",
    "    return (patch - patch.mean())/np.sqrt(patch.var() + eps)\n",
    "\n",
    "def whiten(X):\n",
    "    X_norm = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "    cov = np.cov(X_norm, rowvar=False) \n",
    "    U,S,V = np.linalg.svd(cov)\n",
    "\n",
    "    X_zca = U.dot(np.diag(1.0/np.sqrt(S + 0.1))).dot(U.T).dot(X_norm.T).T\n",
    "    return X_zca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cached(fun, data, name):\n",
    "    return fun(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Fh1an_UgS5aI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293fafb99f914ad3b577c5c65b281c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_patches(data):\n",
    "    patches = []\n",
    "    reshaped = data.reshape(-1,32,32,3)\n",
    "    n = int(patch_num / (32-PATCH_SIZE+1) ** 2 + 1)\n",
    "    for i in tqdm(range(n)):\n",
    "        for r in range(32-PATCH_SIZE+1):\n",
    "            for c in range(32-PATCH_SIZE+1):\n",
    "                patch = reshaped[i][c:(c+PATCH_SIZE),r:(r+PATCH_SIZE)].flatten()\n",
    "                patch_norm = normalize_patch(patch, eps=10)\n",
    "                patches.append(patch_norm)\n",
    "\n",
    "    P = np.vstack(patches)\n",
    "    return whiten(P)\n",
    "\n",
    "P_zca = run_cached(extract_patches, X_trn, 'P_zca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR_rrRlf7YfL",
    "outputId": "5aeaa079-7667-45c6-ae17-4c9510a9d443",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import numpy as np\n",
    "\n",
    "def centroids(data):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, verbose=False, n_init=1, max_iter=200, batch_size=10000)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "filters_final = run_cached(centroids, P_zca, 'kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some intuition what the patches are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_hIoR0m4-6jZ"
   },
   "outputs": [],
   "source": [
    "def dist(x,y):\n",
    "    return np.sqrt((x - y).dot(x-y))\n",
    "\n",
    "def create_patch_features(X):    \n",
    "    X_mapped_list_per_image = []\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        mapped_features = []\n",
    "        for r in range(0, 32-PATCH_SIZE+1, STRIDE):\n",
    "            for c in range(0, 32-PATCH_SIZE+1, STRIDE):\n",
    "                patch = X[i].reshape(32,32,3)[c:(c+PATCH_SIZE),r:(r+PATCH_SIZE)].flatten()\n",
    "                patch_norm = normalize_patch(patch, eps=0.01)\n",
    "                mapped_features.append([dist(patch_norm, f) for f in filters_final])\n",
    "        X_mapped_list_per_image.append(np.vstack(mapped_features))\n",
    "    X_mapped = np.asarray(X_mapped_list_per_image).reshape(-1, ((32-PATCH_SIZE)//STRIDE+1)**2*filters_final.shape[0])\n",
    "    return X_mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OF9ryTgRS5aO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def create_patch_features__vectorized(X):    \n",
    "    X_mapped_list_per_image = []\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        patches = image.extract_patches_2d(X[i], (PATCH_SIZE, PATCH_SIZE))\n",
    "        strided_patches = patches.reshape( 32-PATCH_SIZE+1 , 32-PATCH_SIZE+1, PATCH_SIZE, PATCH_SIZE, 3)[::STRIDE,::STRIDE,:,:,:]\n",
    "        strided_patches = strided_patches.reshape(((32-PATCH_SIZE)//STRIDE+1)**2, PATCH_SIZE * PATCH_SIZE * 3)\n",
    "        mapped_features = euclidean_distances(np.asarray([normalize_patch(patch, eps=0.01) for patch in strided_patches]), filters_final)\n",
    "        X_mapped_list_per_image.append(mapped_features.reshape(((32-PATCH_SIZE)//STRIDE+1)**2 * filters_final.shape[0]))\n",
    "    X_mapped = np.asarray(X_mapped_list_per_image)\n",
    "    return X_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm7-PrElCWp_",
    "outputId": "10a21f52-9390-4148-dbde-cd6c69b969a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0abdca74ab24f68bb6f4da158aa823e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_mapped_trn = run_cached(create_patch_features__vectorized, X_trn, 'X_mapped_trn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8730d9a3a26403d8fb709b2b4381fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_mapped_tst = run_cached(create_patch_features__vectorized, X_tst, 'X_mapped_tst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4MpAk2xS5aS"
   },
   "source": [
    "# Logistic Regression on mapped features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - data.mean(axis=0))/data.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pfs8qf8nS5aQ"
   },
   "outputs": [],
   "source": [
    "#X_mapped_trn_norm = run_cached(normalize, X_mapped_trn, \"X_mapped_trn_norm\")\n",
    "#X_mapped_tst_norm = run_cached(normalize, X_mapped_tst, \"X_mapped_tst_norm\")\n",
    "X_mapped_trn_norm = normalize(X_mapped_trn)\n",
    "X_mapped_tst_norm = normalize(X_mapped_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RtuWcy-S5aT",
    "outputId": "38c4e368-3610-471d-c3aa-b411d7781c71",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        40970     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.15129D+05    |proj g|=  4.73127D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  7.41815D+04    |proj g|=  5.11790D+02\n",
      "\n",
      "At iterate  100    f=  6.52776D+04    |proj g|=  3.59526D+02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "40970    100    104      1     0     0   3.595D+02   6.528D+04\n",
      "  F =   65277.633757993317     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logreg(data):\n",
    "    x, y = data\n",
    "    clf = LogisticRegression(random_state=0, max_iter=100, n_jobs=-1, verbose=True)\n",
    "    clf.fit(x, y.flatten())\n",
    "    return clf\n",
    "\n",
    "clf = run_cached(logreg, (X_mapped_trn_norm, y_trn), 'logreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esPqtlHSS5aU",
    "outputId": "cd063896-e55c-48b9-8de9-a74083c90ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.54722\n",
      "Test:  0.4851\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = clf.predict(X_mapped_trn_norm)\n",
    "print(f\"Train: {(y_trn.flatten() == y_trn_pred).mean()}\")\n",
    "\n",
    "y_tst_pred = clf.predict(X_mapped_tst_norm)\n",
    "print(f\"Test:  {(y_tst.flatten() == y_tst_pred).mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
